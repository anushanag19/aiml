{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Demo \n",
    "\n",
    "There has been an explosion in the volume of unstructured data on the internet in the age of social media. This data is useful and can be used to answer several important business questions, for example sentiment analysis of products or optimizing recommendations for e-commerce platforms. <br>\n",
    "Different methods are used to solve each of these problems as we will see in the Text Mining course. A toolkit which deals with unstructured natural language data very efficiently is nltk. <br> \n",
    "We will see a short demo of some of the most basic functionalities of nltk. <br>\n",
    "For reading more about nltk, you can refer to: https://www.nltk.org/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. Token is a single entity that is building blocks for sentence or paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, how are you doing today?', 'The sky is pinkish-blue.', 'The weather is great, and city is awesome.', 'Hope you have a great day.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Hello, how are you doing today? The sky is pinkish-blue. The weather is great, and city is awesome. Hope you have a great day.\"\n",
    "tokenized_text=sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'sky', 'is', 'pinkish-blue', '.', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'Hope', 'you', 'have', 'a', 'great', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopwords \n",
    "Stopwords are the extremely common words that occur in our corpus with very high frequency. For example, the words 'the', 'a', 'I' are extremely common words in the English language. <br> \n",
    "Hence, during pre-processing for some algorithms we might want to remove these words from our text. That can be done as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'himself', 'aren', 'these', 'hasn', 'can', 'by', 've', 'not', \"you're\", 'themselves', \"shan't\", 'been', 'yourself', 'it', 'against', 'doesn', 'after', 'him', 'has', 'wouldn', \"shouldn't\", 'down', \"you'll\", 'other', 'more', \"aren't\", 'couldn', 'hadn', \"haven't\", 'ours', 'very', 'some', 'for', 'haven', 'o', 'mightn', 'nor', 'didn', 'is', 'through', 's', 'were', 'same', \"didn't\", 'needn', 'no', \"mustn't\", 'myself', \"couldn't\", \"wasn't\", 'if', 'just', \"don't\", 'yourselves', 'each', 'how', 'that', 'our', 'we', 'again', 'out', 'most', 'an', 'but', 'me', 'wasn', 'up', 'to', \"hasn't\", 'at', 'when', 'off', 'them', 'should', 'few', 'below', 'doing', 'own', 'was', 'those', 'then', 'had', 'over', 'a', 'does', 'don', 'mustn', 'will', 'now', \"weren't\", 'i', 'ourselves', 'shan', 'any', 'ain', \"she's\", 'yours', 'too', 'he', 'this', 'because', 'than', 'who', 'further', 'its', 'are', 'won', 'here', 'weren', 'do', \"should've\", 'such', 'during', 'as', 'only', 'their', 'whom', \"that'll\", 'so', 'y', 'ma', 'hers', 'theirs', \"you'd\", 'why', 'her', 'having', 'or', 't', 'll', 'which', 'both', 'above', 'she', 'into', 'there', \"isn't\", \"mightn't\", 'am', 'his', 'my', 'be', 'until', 're', 'about', 'between', 'd', 'while', 'have', 'from', 'all', 'you', 'itself', 'before', \"won't\", \"you've\", \"needn't\", \"doesn't\", 'the', 'they', 'of', 'isn', \"wouldn't\", 'once', 'in', 'shouldn', 'herself', 'what', 'and', 'your', 'where', 'being', 'did', 'under', \"it's\", 'with', 'm', \"hadn't\", 'on'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filterd Text: ['Hello', ',', 'today', '?', 'The', 'sky', 'pinkish-blue', '.', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'Hope', 'great', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_text=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_text.append(w)\n",
    "print(\"Filterd Text:\",filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming \n",
    "\n",
    "Normalization considers another type of noise in the text. For example, creation, creating, created, all words reduce to a common word \"create\". It reduces derivationally related forms of a word to a common root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer: Stemmed words: ['connect', 'connect', 'connect']\n"
     ]
    }
   ],
   "source": [
    "related_words=['connecting','connected','connection']\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in related_words:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"PorterStemmer: Stemmed words:\",stemmed_words_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis. Lemmatization is usually more sophisticated than stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: flying\n",
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word))\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 22 samples and 30 outcomes>\n",
      "[('is', 3), ('.', 3), (',', 2), ('you', 2), ('The', 2)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist)\n",
    "print(fdist.most_common(5))\n",
    "# samples indicates the number of unique elements in the text\n",
    "# outcomes indicates the total number of elements in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. POS tagging\n",
    "The primary target of POS tagging is to label each word in the text with a Part-Of-Speech label. The labels assigned are from [NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS] etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'a', 'rapidly', 'developing', 'field', '.']\n"
     ]
    }
   ],
   "source": [
    "sent=\"NLP is a rapidly developing field.\"\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLP', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('rapidly', 'RB'),\n",
       " ('developing', 'JJ'),\n",
       " ('field', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
