{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION:\n",
    "\n",
    "\n",
    " - The goal of this miniproject is to recommend the best online store in terms of price for users after comparing the price across the stores for various FMCG products. \n",
    " - The data about the products is obtained via Web scrapping. \n",
    "    \n",
    ">Group Name: __DATA SCIENTISTS__ <br>\n",
    ">Group Members: __Soujanya M, Ramya B, Suresh R__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERALL APPROACH: \n",
    "\n",
    "The four tasks that will be done as a part of this exercise will be as follows:\n",
    " \n",
    "    A. Data Acquisition  \n",
    "    B. Data Cleaning \n",
    "    C. Data Integration \n",
    "    D. Exploratory Data Analysis and Recommendation\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "## Approach:\n",
    "\n",
    " 1.  Identify set of categories and products that we want to scrap from various online e-commerce platforms.\n",
    "     >There are around 30 categories and 150 items identified for web scrapping\n",
    " 2.  Choose e-commerce platforms \n",
    "     >Following platforms have been chosen for scrapping\n",
    "     - Big Basket\n",
    "     - DMart\n",
    "     - Grofers\n",
    " 3.  Search the products on the browser and observe the URL/AJAX calls made as a part of fetching data from server\n",
    " 4.  Identify various CSS classes for product attributes like Product name, MRP, Special Price\n",
    " 5.  Write code to perform web scrapping for each of the platforms and store the results in a separate CSV file for later use\n",
    "     > The results are stored in respective csv files as follows\n",
    "     - big_basket.csv\n",
    "     - dmart.csv\n",
    "     - grofers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FMCG Categories and Products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Library Imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories to be scrapped  30\n",
      "Total products to be scrapped  142\n"
     ]
    }
   ],
   "source": [
    "#The input file items.csv contains various product categories and brands for which scrapping has to be performed\n",
    "df_products = pd.read_csv(\"products.csv\" )\n",
    "print(\"Total categories to be scrapped \", len(df_products[\"Product\"].unique()))  \n",
    "print(\"Total products to be scrapped \", df_products.shape[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping Bigbasket\n",
    "\n",
    " - The URL for big basket data is different for first page and subsequent pages. \n",
    " - So, we need to maintain two separate URLs in the code\n",
    " - Since, we are very specific about each product, scrapping 2 pages is more than enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Big Basket Scrapping\n",
    "def search_bigbasket(search_brand, search_product):\n",
    "    \n",
    "    #Max pages to scrap\n",
    "    max_pages = 2\n",
    "    \n",
    "    #Product full name\n",
    "    searchStr = search_brand + \" \" + search_product\n",
    "    \n",
    "    #URL for page 1\n",
    "    url_page_1 = \"https://www.bigbasket.com/custompage/getsearchdata/?type=deck&slug={0}\"\n",
    "    \n",
    "    #URL for subsequent pages\n",
    "    url_for_page_n = \"https://www.bigbasket.com/product/get-products/?slug={0}&page={1}&tab_type=[%22all%22]&sorted_on=relevance&listtype=ps\"\n",
    "    \n",
    "    #Replace spaces in the product name with +\n",
    "    search_str_encoded = searchStr.replace(\" \",\"+\")\n",
    "    \n",
    "    #Form actual URL for page 1\n",
    "    url = url_page_1.format(search_str_encoded)\n",
    " \n",
    "    #Necessary request headers\n",
    "    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "    \n",
    "    #Get the response for page 1\n",
    "    productsInfo = requests.get(url, headers=headers).json()\n",
    "    \n",
    "    #Extract product data into a dictionary object\n",
    "    products =  productsInfo['json_data']['tab_info'][0]['product_info']['products']\n",
    "    product_data=[]\n",
    "    for product in  products:\n",
    "        options = product['all_prods']\n",
    "        if(not options): \n",
    "            product_data.append({\"search_brand\": search_brand, \"search_product\": search_product, \"shop\": \"Big Basket\", \"product_name\": str(product['p_brand']).strip() + \" \" + str(product['p_desc']).strip(), \"weight\":product['w'], \"mrp\": product['mrp'], \"special_price\": product['sp'] })\n",
    "            continue\n",
    "        for option  in product['all_prods']:\n",
    "            product_data.append({\"search_brand\": search_brand, \"search_product\": search_product, \"shop\": \"Big Basket\",  \"product_name\": str(product['p_brand']).strip() + \" \" + str(product['p_desc']).strip(), \"weight\":option['w'],\n",
    "                                 \"mrp\": option['mrp'], \"special_price\": option['sp'] }) \n",
    " \n",
    "    \n",
    "    #Do it for subsequent pages     \n",
    "    for page in range(2,max_pages):\n",
    "        newurl = url_for_page_n.format(search_str_encoded, page) \n",
    "        productsInfo = requests.get(newurl, headers=headers).json()\n",
    "        products =  productsInfo['tab_info']['product_map']['all']['prods']\n",
    "\n",
    "        for product in  products:\n",
    "            options = product['all_prods']\n",
    "            if(not options): \n",
    "                product_data.append({\"search_brand\": search_brand, \"search_product\": search_product, \"shop\": \"Big Basket\",    \"product_name\": str(product['p_brand']).strip() + \" \" + str(product['p_desc']).strip(), \"weight\":product['w'], \"mrp\": product['mrp'], \"special_price\": product['sp'] })\n",
    "                continue\n",
    "            for option  in product['all_prods']:\n",
    "                product_data.append({\"search_brand\": search_brand, \"search_product\": search_product,  \"shop\": \"Big Basket\",  \"product_name\": str(product['p_brand']).strip() + \" \" + str(product['p_desc']).strip(), \"weight\":option['w'], \"mrp\": option['mrp'], \"special_price\": option['sp'] })\n",
    "\n",
    "    #Return the scrapped data           \n",
    "    return product_data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping DMart\n",
    "\n",
    " - The URL for DMart is same for all pages as long as we include the page number. \n",
    " - So, we need to maintain a URL with placeholder for page in the code\n",
    " - DMart requires a Store Id be sent in the header to be sent. Otherwise, it will return wrong results\n",
    " - Since, we are very specific about each product, scrapping 2 pages is more than enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dmart Scrapping\n",
    "def search_dmart(search_brand, search_product):\n",
    "    #Max pages to scrap\n",
    "    max_pages = 2\n",
    "    #Product full name\n",
    "    searchStr = search_brand + \" \" + search_product\n",
    "    #URL for page \n",
    "    url_page  = \"https://digital.dmart.in/api/v1/search/{0}?page={1}\"\n",
    "    #Necessary request headers\n",
    "    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "    \n",
    "    #DMart requires store Id \n",
    "    headers['storeId']='10657'\n",
    "    #Replace spaces in the product name with +\n",
    "    search_str_encoded = searchStr.replace(\" \",\"+\")\n",
    "    \n",
    "    #Extract product data into a dictionary object\n",
    "    product_data=[]\n",
    "    for page in range(1,max_pages):\n",
    "        url = url_page.format(search_str_encoded, page)\n",
    "        productsInfo = requests.get(url, headers=headers).json()\n",
    "        #If the scrapping did not yield any products, just continue with next product\n",
    "        products=productsInfo.get( 'suggestionView')\n",
    "        if(not products):\n",
    "            continue\n",
    "     \n",
    "        for product in  products:\n",
    "            skus = product['skus']\n",
    "            for sku in skus:  \n",
    "                if(sku['defining']):\n",
    "                    name =   sku.get( 'name',  product['name']).strip()\n",
    "                    product_data.append({\"search_brand\": search_brand, \"search_product\": search_product, \"shop\": \"DMart\",   \"product_name\": name,\"weight\": sku['defining'][0]['volume'], \"mrp\": sku['price_MRP'], \"special_price\": sku['price_SALE'] })\n",
    "    #Return the scrapped data           \n",
    "    return product_data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping Grofers\n",
    "\n",
    " - The URL for Grofers is same for all pages as long as we include the page number. \n",
    " - Grofers requires a special header to be sent. Otherwise, it won't return any results\n",
    " - So, we need to maintain a URL with placeholder for page in the code\n",
    " - Since, we are very specific about each product, scrapping 2 pages is more than enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grofers Scrapping\n",
    "def search_grofers(search_brand, search_product):\n",
    "    #Max pages to scrap\n",
    "    max_pages = 2\n",
    "    #Product full name\n",
    "    searchStr = search_brand + \" \" + search_product\n",
    "    #URL for page. This should include Longitude and Latitude as well. \n",
    "    url_page  = \"https://grofers.com/v5/search/merchants/26659/products/?lat=17.4196281427546&lon=78.3790778036223&q={0}&suggestion_type=0&t=1&start={1}&size=48\"\n",
    "    #Necessary request headers\n",
    "    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "    #Grofers requires this header \n",
    "    headers['app_client'] = 'consumer_web' \n",
    "    #Replace spaces in the product name with +\n",
    "    search_str_encoded = searchStr.replace(\" \",\"+\")\n",
    "    #Extract product data into a dictionary object\n",
    "    product_data=[]\n",
    "    for page in range(1,max_pages):\n",
    "        start_pos = (page-1)*48\n",
    "        if(start_pos==0):\n",
    "            start_pos = 1\n",
    "        #Create page specific URL\n",
    "        url = url_page.format(search_str_encoded, start_pos)\n",
    "        productsInfo = requests.get(url, headers=headers).json()\n",
    "        #If the scrapping did not yield any products, just continue with next product\n",
    "        products= productsInfo.get('products')\n",
    "        if(not products):\n",
    "            break\n",
    "        for product in  products:\n",
    "            prod_variants = product['variant_info']\n",
    "            for var in prod_variants:\n",
    "                product_data.append({\"search_brand\": search_brand, \"search_product\": search_product,   \"shop\": \"Grofers\",  \"product_name\": var[\"line_1\"],\"weight\": var[\"unit\"], \"mrp\": var[\"mrp\"], \"special_price\": var[\"price\"]})\n",
    "    #Return the scrapped data    \n",
    "    return product_data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Scrapping for all online portals\n",
    "\n",
    " - The following section performs scrapping for all the 3 portals for the products mentioned in input file. \n",
    " - Ensure input.csv file is present in the current directory before calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping for all sites\n",
    "def perform_scrapping():\n",
    "    #Read all items for which scrapping is needed\n",
    "    df_input   = pd.read_csv(\"items.csv\" )\n",
    "    #Define an empty dataframe to store results for each of the shops\n",
    "    column_names =   [\"search_brand\",\"search_product\", \"shop\",   \"product_name\", \"weight\", \"mrp\", \"special_price\"]\n",
    "    \n",
    "    df_bigbasket = pd.DataFrame(columns = column_names)\n",
    "    df_dmart = pd.DataFrame(columns = column_names)\n",
    "    df_grofers = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    #For each of the products, peform scrapping\n",
    "    for ( idx , search_brand, search_product) in df_input.itertuples():\n",
    "  \n",
    "        print('Scrapping BigBasket for ', idx, search_brand, search_product)\n",
    "        product_data = search_bigbasket(search_brand, search_product)\n",
    "        df_bigbasket = df_bigbasket.append(product_data[:5])\n",
    "\n",
    "        print('Scrapping Grofers for ', idx, search_brand, search_product)\n",
    "        product_data = search_grofers(search_brand, search_product)\n",
    "        df_grofers = df_grofers.append(product_data[:5])\n",
    "\n",
    "        print('Scrapping BigBasket for ', idx, search_brand, search_product)\n",
    "        product_data = search_dmart(search_brand, search_product)\n",
    "        df_dmart = df_dmart.append(product_data[:5])\n",
    "  \n",
    "    #Store final results for each of the stores in separate csv file\n",
    "    df_bigbasket.to_csv('big_basket.csv', index=False,encoding='utf-8')\n",
    "    df_grofers.to_csv('grofers.csv', index=False, encoding='utf-8')\n",
    "    df_dmart.to_csv('dmart.csv', index=False, encoding='utf-8')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping Results\n",
    "This section shows the results of the scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping BigBasket for  0 3 Roses Tea\n",
      "Scrapping Grofers for  0 3 Roses Tea\n",
      "Scrapping BigBasket for  0 3 Roses Tea\n",
      "Scrapping BigBasket for  1 7 Up Soft Drink\n",
      "Scrapping Grofers for  1 7 Up Soft Drink\n",
      "Scrapping BigBasket for  1 7 Up Soft Drink\n",
      "Scrapping BigBasket for  2 ACT II Popcorn\n",
      "Scrapping Grofers for  2 ACT II Popcorn\n",
      "Scrapping BigBasket for  2 ACT II Popcorn\n",
      "Scrapping BigBasket for  3 Adidas Deodrant\n",
      "Scrapping Grofers for  3 Adidas Deodrant\n",
      "Scrapping BigBasket for  3 Adidas Deodrant\n",
      "Scrapping BigBasket for  4 Anurag Cooking Oil\n",
      "Scrapping Grofers for  4 Anurag Cooking Oil\n",
      "Scrapping BigBasket for  4 Anurag Cooking Oil\n",
      "Scrapping BigBasket for  5 Appy Fizz Juice\n",
      "Scrapping Grofers for  5 Appy Fizz Juice\n",
      "Scrapping BigBasket for  5 Appy Fizz Juice\n",
      "Scrapping BigBasket for  6 Aswini Hair Oil\n",
      "Scrapping Grofers for  6 Aswini Hair Oil\n",
      "Scrapping BigBasket for  6 Aswini Hair Oil\n",
      "Scrapping BigBasket for  7 Bisleri Mineral Water\n",
      "Scrapping Grofers for  7 Bisleri Mineral Water\n",
      "Scrapping BigBasket for  7 Bisleri Mineral Water\n",
      "Scrapping BigBasket for  8 Boost Soft Drink\n",
      "Scrapping Grofers for  8 Boost Soft Drink\n",
      "Scrapping BigBasket for  8 Boost Soft Drink\n",
      "Scrapping BigBasket for  9 Britannia Rusks\n",
      "Scrapping Grofers for  9 Britannia Rusks\n",
      "Scrapping BigBasket for  9 Britannia Rusks\n",
      "Scrapping BigBasket for  10 Brooke Bond Tea\n",
      "Scrapping Grofers for  10 Brooke Bond Tea\n",
      "Scrapping BigBasket for  10 Brooke Bond Tea\n",
      "Scrapping BigBasket for  11 Bru Coffee\n",
      "Scrapping Grofers for  11 Bru Coffee\n",
      "Scrapping BigBasket for  11 Bru Coffee\n",
      "Scrapping BigBasket for  12 Cadbury Dairy milk Chocolate\n",
      "Scrapping Grofers for  12 Cadbury Dairy milk Chocolate\n",
      "Scrapping BigBasket for  12 Cadbury Dairy milk Chocolate\n",
      "Scrapping BigBasket for  13 Cadbury Five star Chocolate\n",
      "Scrapping Grofers for  13 Cadbury Five star Chocolate\n",
      "Scrapping BigBasket for  13 Cadbury Five star Chocolate\n",
      "Scrapping BigBasket for  14   Chana Dal\n",
      "Scrapping Grofers for  14   Chana Dal\n",
      "Scrapping BigBasket for  14   Chana Dal\n",
      "Scrapping BigBasket for  15 Cinthol Bath Soap\n",
      "Scrapping Grofers for  15 Cinthol Bath Soap\n",
      "Scrapping BigBasket for  15 Cinthol Bath Soap\n",
      "Scrapping BigBasket for  16 Cinthol Deodrant\n",
      "Scrapping Grofers for  16 Cinthol Deodrant\n",
      "Scrapping BigBasket for  16 Cinthol Deodrant\n",
      "Scrapping BigBasket for  17 Clinic Plus Shampoo\n",
      "Scrapping Grofers for  17 Clinic Plus Shampoo\n",
      "Scrapping BigBasket for  17 Clinic Plus Shampoo\n",
      "Scrapping BigBasket for  18 Close Up Tooth Paste\n",
      "Scrapping Grofers for  18 Close Up Tooth Paste\n",
      "Scrapping BigBasket for  18 Close Up Tooth Paste\n",
      "Scrapping BigBasket for  19 Coca Cola Soft Drink\n",
      "Scrapping Grofers for  19 Coca Cola Soft Drink\n",
      "Scrapping BigBasket for  19 Coca Cola Soft Drink\n",
      "Scrapping BigBasket for  20 Colgate Active Salt Tooth Paste\n",
      "Scrapping Grofers for  20 Colgate Active Salt Tooth Paste\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a7002cea4803>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Call scrapping routine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mperform_scrapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#Load scrapped data into data frames and look at the metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_bb\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"big_basket.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_dmart\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dmart.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-7bf1f2e690fa>\u001b[0m in \u001b[0;36mperform_scrapping\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Scrapping Grofers for '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_brand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_product\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mproduct_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_grofers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_brand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_product\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mdf_grofers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_grofers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Scrapping BigBasket for '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_brand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_product\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   7106\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7107\u001b[0m                 \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7108\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7109\u001b[0m             \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Call scrapping routine\n",
    "perform_scrapping()\n",
    "#Load scrapped data into data frames and look at the metrics\n",
    "df_bb  = pd.read_csv(\"big_basket.csv\")\n",
    "df_dmart  = pd.read_csv(\"dmart.csv\")\n",
    "df_grofers  = pd.read_csv(\"grofers.csv\")\n",
    " \n",
    "print(\"Bigbasket returned products count:\" , df_bb.shape[0] )\n",
    "print(\"DMart returned products count:\" , df_dmart.shape[0] )\n",
    "print(\"Grofers returned products count:\" , df_grofers.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Approach:\n",
    "\n",
    " For each of thep files generated in the previous section, perform following cleanup steps\n",
    " 1. Separate weight and measure to separate columns from weight column\n",
    " 2. Calculate the derived column discount based on MRP and Special Price\n",
    " 3. Drop duplicate products that may have arrived due to wrong search results given by the site\n",
    " 4. Re-arrange columns in proper order\n",
    " 5. Save final set of products in respective csv files as follows\n",
    "     - big_basket_cleaned.csv\n",
    "     - dmart_cleaned.csv\n",
    "     - grofers_cleaned.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up for Big Basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(\"big_basket.csv\",  engine='python')\n",
    "print(df.product_name.unique())\n",
    "print(df.weight.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Inference: Clean up is required as follows.</font>\n",
    "\n",
    " - There are products with combo packs. So, remove them as identifying individual product becomes tough during recommendation\n",
    " - Remove products with weights that has invalid characters other than numbers\n",
    " - Drop duplicate products that may have arrived due to wrong search results given by the site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup routine for bigbasket\n",
    "def clean_bigbasket_data():\n",
    "    #Read the dataset\n",
    "    df  = pd.read_csv(\"big_basket.csv\",  engine='python')\n",
    "    # Pattern to identify combo packs\n",
    "    searchfor = ['x', 'X','\\+','Comb',  'each', '\\(']\n",
    "    #Drop the rows with above patterns\n",
    "    df.drop( df[ df[\"weight\"].str.contains('|'.join(searchfor)) ].index,inplace=True  )\n",
    "    #Separate weight in to weight, measure\n",
    "    df[['weight','measure']] = df.weight.str.split(expand=True) \n",
    "    #Remove rows that have invalid weight entries\n",
    "    df.drop( df[~df[\"weight\"].str.replace('.','',1).str.isnumeric() ].index,inplace=True  )\n",
    "    #Check if product name has the keyword we searched for\n",
    "    df['good_product_ind'] = [x[1] in x[0] for x in zip(df['product_name'], df['search_brand'])]\n",
    "    #Drop the products that don;t meet the above criteria\n",
    "    df.drop( df[~df[\"good_product_ind\"] ].index,inplace=True  )   \n",
    "    #Creae weight_measure column by combining weight and measure\n",
    "    df[ 'weight_measure' ] = df.weight.str.strip() + df.measure.str.strip()\n",
    "    #Re-order columns\n",
    "    df = df[['search_product','search_brand', 'shop' ,'product_name','weight', 'measure', 'weight_measure', 'mrp','special_price']]\n",
    "    #Calculate discount\n",
    "    df['discount'] = (df.mrp - df.special_price) *100 / df.mrp \n",
    "    #Remove duplicate entries for given product and weight\n",
    "    df.drop_duplicates(subset=['product_name','weight'], keep='last', inplace=True)\n",
    "    #Save the results to big_basket_cleaned\n",
    "    df.to_csv('big_basket_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up for DMart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(\"dmart.csv\",  engine='python')\n",
    "print(df.product_name.unique())\n",
    "print(df.weight.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Inference: Clean up is required as follows.</font>\n",
    "\n",
    " - There are products with combo packs. So, remove them as identifying individual product becomes tough during recommendation\n",
    " - Remove products with weights that has invalid characters other than numbers\n",
    " - Drop duplicate products that may have arrived due to wrong search results given by the site\n",
    " - Product name contains weight. Remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup routine for dmart\n",
    "def clean_dmart_data():\n",
    "    #Read the dataset\n",
    "    df  = pd.read_csv(\"dmart.csv\")\n",
    "    # Pattern to identify combo packs\n",
    "    searchfor = ['x', 'X', 'W', 'Bags','Sachets','Drops','Wipes','U','Wipe','Pellets','Cubes','unit','units','tablets',\"\\+\",'XL','L','M','S','Set','Pink']\n",
    "    #Drop the rows with above patterns\n",
    "    df.drop( df[ df[\"weight\"].str.contains('|'.join(searchfor)) ].index,inplace=True  )\n",
    "    #Drop the rows with zero weights\n",
    "    df.drop( df[ df[\"weight\"] == \"0\" ].index,inplace=True  )\n",
    "    #Separate weight in to weight, measure\n",
    "    df[['weight','measure']] = df.weight.str.split(expand=True) \n",
    "    #Replace gm meausre to g to make it consistent with other shops \n",
    "    df[ 'measure' ] = df.measure.str.replace('gm','g') \n",
    "    #Remove rows that have invalid weight entries\n",
    "    df.drop( df[~df[\"weight\"].str.replace('.','',1).str.isnumeric() ].index,inplace=True  )\n",
    "    #Creae weight_measure column by combining weight and measure\n",
    "    df[ 'weight_measure' ] = df.weight.str.strip() + df.measure.str.strip()\n",
    "    #Check if product name has the keyword we searched for\n",
    "    df['good_product_ind'] = [x[1] in x[0] for x in zip(df['product_name'], df['search_brand'])]\n",
    "    #Drop the products that don;t meet the above criteria\n",
    "    df.drop( df[~df[\"good_product_ind\"] ].index,inplace=True  )   \n",
    "    #Re-order columns\n",
    "    df = df[['search_product','search_brand', 'shop' ,'product_name','weight', 'measure', 'weight_measure', 'mrp','special_price']]\n",
    "    #Calculate discount\n",
    "    df['discount'] = (df.mrp - df.special_price) *100 / df.mrp \n",
    "    #Replace the measure which is part of the product name with blank\n",
    "    df['product_name'] = df.product_name.str.split(\":\").str[0].str.strip()\n",
    "    #Remove duplicate entries for given product and weight\n",
    "    df.drop_duplicates(subset=[  'product_name','weight' ],keep='last', inplace=True)\n",
    "    #Save the results to dmart_cleaned\n",
    "    df.to_csv('dmart_cleaned.csv', index=False)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up for Grofers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(\"grofers.csv\",  engine='python')\n",
    "print(df.product_name.unique())\n",
    "print(df.weight.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Inference: Clean up is required as follows.</font>\n",
    "\n",
    " - There are products with combo packs. So, remove them as identifying individual product becomes tough during recommendation\n",
    " - Remove products with weights that has invalid characters other than numbers\n",
    " - Drop duplicate products that may have arrived due to wrong search results given by the site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup routine for grofers\n",
    "def clean_grofers_data():\n",
    "    #Read the dataset\n",
    "    df  = pd.read_csv(\"grofers.csv\")\n",
    "    # Pattern to identify combo packs\n",
    "    searchfor = ['x', 'X', ',', 'Refills', 'Bags','Sachets','Drops','Wipes','U','Wipe','Pellets','Cubes','unit','units','tablets',\"\\+\",'XL','L','M','S','Set','Pink']\n",
    "    #Drop the rows with above patterns\n",
    "    df.drop( df[ df[\"weight\"].str.contains('|'.join(searchfor)) ].index,inplace=True  )\n",
    "    #Separate weight in to weight, measure\n",
    "    df[['weight','measure']] = df.weight.str.split(expand=True) \n",
    "    #Remove rows that have invalid weight entries\n",
    "    df.drop( df[~df[\"weight\"].str.replace('.','',1).str.isnumeric() ].index,inplace=True  )\n",
    "    #Creae weight_measure column by combining weight and measure\n",
    "    df[ 'weight_measure' ] = df.weight.str.strip() + df.measure.str.strip()\n",
    "    #Check if product name has the keyword we searched for\n",
    "    df['good_product_ind'] = [x[1] in x[0] for x in zip(df['product_name'], df['search_brand'])]\n",
    "    #Drop the products that don;t meet the above criteria\n",
    "    df.drop( df[~df[\"good_product_ind\"] ].index,inplace=True  )    \n",
    "    #Re-order columns\n",
    "    df = df[['search_product','search_brand', 'shop' ,'product_name','weight', 'measure', 'weight_measure', 'mrp','special_price']]\n",
    "    #Calculate discount\n",
    "    df['discount'] = (df.mrp - df.special_price) *100 / df.mrp \n",
    "    #Remove duplicate entries for given product and weight\n",
    "    df.drop_duplicates(subset=[  'product_name','weight'],keep='last', inplace=True)\n",
    "    #Save the results to grofers_cleaned\n",
    "    df.to_csv('grofers_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up for All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run cleanup \n",
    "clean_bigbasket_data()\n",
    "clean_dmart_data()\n",
    "clean_grofers_data()\n",
    "\n",
    "df_bb  = pd.read_csv(\"big_basket_cleaned.csv\")\n",
    "df_dmart  = pd.read_csv(\"dmart_cleaned.csv\")\n",
    "df_grofers  = pd.read_csv(\"grofers_cleaned.csv\")\n",
    " \n",
    "print(\"Bigbasket products count after cleanup:\" , df_bb.shape[0] )\n",
    "print(\"DMart products count after cleanup:\" , df_dmart.shape[0] )\n",
    "print(\"Grofers products count after cleanup:\" , df_grofers.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration\n",
    "\n",
    "## Approach:\n",
    "\n",
    " For each of the files generated in the previous section, combine them into a single file as follows\n",
    " 1. Load following scrapped and already cleaned files into separate data frames \n",
    " >\n",
    "     - big_basket_cleaned.csv\n",
    "     - dmart_cleaned.csv\n",
    "     - grofers_cleaned.csv \n",
    "     >\n",
    "         \n",
    " 2. Join all the 3 data frames into a single data frame\n",
    " 3. Sort the products based on product type and name so that related products will be together irrespective of he shop\n",
    " \n",
    " 4. Write the data frame into a final file\n",
    "      - combined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine cleaned files into combined.csv file\n",
    "def combine_products():\n",
    "    #Load cleaned files into separate dataframes \n",
    "    df_dmart  = pd.read_csv(\"dmart_cleaned.csv\")\n",
    "    df_grofers  = pd.read_csv(\"grofers_cleaned.csv\")\n",
    "    df_bb  = pd.read_csv(\"big_basket_cleaned.csv\")\n",
    "   \n",
    "    #Combine these files into a single data frame\n",
    "    df_combined = pd.concat([df_bb, df_dmart, df_grofers])\n",
    "    \n",
    "    #Sort them based on searched product and resultant product name\n",
    "    df_combined.sort_values(by=[ 'search_product' , 'product_name'], inplace=True)\n",
    "    #Reset the index\n",
    "    df_combined = df_combined.reset_index(drop=True)\n",
    "    #Store the results in final file combined.csv\n",
    "    df_combined.to_csv('combined.csv', index=False)\n",
    "     \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Products \n",
    "combine_products()\n",
    "#Read combined products into data frame\n",
    "df_combined  = pd.read_csv(\"combined.csv\")\n",
    "print(\"Final Scrapped and Cleaned Product Count:\" , df_combined.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA & RECOMMENDATION\n",
    "\n",
    "## Approach:\n",
    "\n",
    " For each of the files generated in the previous section, combine them into a single file as follows\n",
    " 1. Load following scrapped, cleaned and combined file into a data frame \n",
    " \n",
    "     - combined.csv\n",
    "   \n",
    "         \n",
    " 2. Perform EDA\n",
    " 3. Provide user interface for users to choose a product category, name and size, and recommended from which shop the user can buy and the discount  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file into a dataframe\n",
    "items = pd.read_csv('combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print shape of the combined dataset\n",
    "print(\"The total count of products available across shops \",  items.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the first few rows\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are any null values in the final dataset\n",
    "items.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which shops are part of the dataset\n",
    "items.shop.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique product categories\n",
    "items.search_product.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot for all attributes\n",
    "sns.pairplot(items)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap for all attributes\n",
    "plt.figure(figsize = (10,8))\n",
    "corr = items.corr()\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference: The above heat map shows there is a tight correlation between mrp and special_price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many products does each shop have to offer?\n",
    "prods_shops_df = items[['shop','search_product']]\n",
    "prod_shops_counts =prods_shops_df.groupby(['shop'],as_index =False).count()\n",
    "prod_shops_counts.plot.bar(x=\"shop\", y=\"search_product\", rot=70, title=\"Products accross Vendors\");\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference: Big Basket has highest number of products available while Grofers has the least. Customers are most likely to find a product in Big Basket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many products avaialble for each product category overall\n",
    "products_count =prods_shops_df.groupby(['search_product'],as_index =False).count()\n",
    "dataFrame = pd.DataFrame(data=products_count);\n",
    "y_pos = np.arange(len(dataFrame))\n",
    "# Draw a vertical bar chart\n",
    "ax = dataFrame.plot.bar(x=\"search_product\", y=\"shop\", rot=80, title=\"Products and their Availability\");\n",
    "ax.set_xlabel(\"Product\")\n",
    "ax.set_ylabel(\"Count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Number of products sold per product category by each shop \n",
    "pd.crosstab(items.shop, items.search_product, margins=True, margins_name=\"Total\",rownames=['shop']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Which Individual Products  have Discount\n",
    "discounts_df = items[ (items.discount > 0.0)]\n",
    "  \n",
    "discounts_shop_products = discounts_df[['search_product','shop']]\n",
    "discounts_per_shop =discounts_shop_products.groupby(['shop'],as_index =False).count()\n",
    "ax = discounts_per_shop.plot.bar(x=\"shop\", y=\"search_product\", rot=70, title=\"Products accross Vendors with discount\");\n",
    "ax.set_xlabel(\"Shop\")\n",
    "ax.set_ylabel(\"Product count with discount\") \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference: DMart offers the most discounts. So customers looking for discounts should visit DMart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Note: Ensure combined.csv file is present in current path before executing following recommendation code.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries for user interface\n",
    "from ipywidgets import interact, Dropdown, HTML, Layout, Box, Label\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Read data into dataframe\n",
    "df_combined = pd.read_csv(\"combined.csv\")\n",
    "\n",
    "#Unique product types that the user can buy\n",
    "products_list = df_combined[\"search_product\"].unique()     \n",
    "\n",
    "#Create dropdown widgets for the user to interact\n",
    "product_type_dropdown = Dropdown(description=\"Product Type:\", options = products_list)\n",
    "product_names_dropdown = Dropdown(description=\"Product:\")\n",
    "product_sizes_dropdown = Dropdown(description=\"Size:\")\n",
    "recommendation_html = HTML(\n",
    "    value=\" \",\n",
    "    placeholder='',\n",
    "    description='',  layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "box = Box(\n",
    "    [\n",
    "        Label(value='Recommendation:'),\n",
    "        recommendation_html\n",
    "    ]\n",
    ") \n",
    "#Event handler when product category changes\n",
    "def update_product_name_options(change):  \n",
    "    product_type = product_type_dropdown.value         \n",
    "    df_product = df_combined[  df_combined.search_product.str.lower().str.contains(product_type.lower())    ]\n",
    "    products = df_product[\"product_name\"].unique()\n",
    "    product_names_dropdown.options = products\n",
    "product_type_dropdown.observe(update_product_name_options )\n",
    " \n",
    "#Event handler when product name changes\n",
    "def update_product_size_options(change): \n",
    "    if change['name'] == 'value' and (change['new'] != change['old']):\n",
    "        product_type = product_type_dropdown.value\n",
    "        product_name = product_names_dropdown.value\n",
    "        df_product = df_combined[  df_combined.search_product.str.lower().str.contains(product_type.lower())    ]\n",
    "        product_name = product_name.replace(\"+\", \"\\+\")\n",
    "        df_product = df_product[  df_product.product_name.str.lower().str.contains(product_name.lower())   ] \n",
    "        product_sizes = df_product[\"weight_measure\"].unique()\n",
    "        product_sizes_dropdown.options = product_sizes\n",
    "product_names_dropdown.observe(update_product_size_options )\n",
    "\n",
    "#Event handler when product size changes\n",
    "def recommend_product(change):\n",
    "    if change['name'] == 'value' and (change['new'] != change['old']):\n",
    "        product_type = product_type_dropdown.value\n",
    "        product_name = product_names_dropdown.value\n",
    "        product_size = product_sizes_dropdown.value\n",
    "        recommendation_html.value=\"\"\n",
    "        product_name = product_name.replace(\"+\", \"\\+\")\n",
    "        if(product_size):\n",
    "            df_final_list = df_combined[ df_combined.search_product.str.lower().str.contains(product_type.lower()) & \n",
    "                                         df_combined.product_name.str.lower().str.contains(product_name.lower()) & \n",
    "                                         df_combined.weight_measure.str.lower().str.contains(product_size.lower()) ] \n",
    "            df_final_list.sort_values(by='special_price', inplace=True)\n",
    "            discount = round(  df_final_list[ \"discount\"].iloc[0] , 2)\n",
    "            recommendation_html.value= \"Buy it from <b>\" + df_final_list.shop.iloc[0] + \"</b> for Rs.\"  + str(df_final_list[ \"special_price\"].iloc[0])  + \" discount of \" + str(discount) + \"%\"\n",
    "product_sizes_dropdown.observe(recommend_product )\n",
    "\n",
    "#Display all user interface elements \n",
    "display(product_type_dropdown)\n",
    "display(product_names_dropdown)\n",
    "display(product_sizes_dropdown)\n",
    "display(box)\n",
    "update_product_name_options(None )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
