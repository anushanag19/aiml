{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FE MINI PROEJCT: DS CHAMPS: WEB SCRAPING\n",
    "\n",
    "Team :Anusha Nagineni, Dharmendra Dash, Adiseshulu Pujari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Project Description:\n",
    "We are web scraping Daily Essential products from 3 e-commerce websites (DMart, Grofers and BigBasket) and provide best recommendation for the products.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Below 4 tasks will be performed in this project.\n",
    " \n",
    "    A. Data Acquisition  \n",
    "    B. Data Cleaning \n",
    "    C. Data Integration \n",
    "    D. Exploratory Data Analysis and Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Data Acquisition : We fetch product data from e-commerce websites using Web Scraping. \n",
    "\n",
    "Steps followed to Web Scrape:\n",
    " 1.  Identify products that we want to scrap from our chosen 3 online e-commerce platforms.\n",
    " 2.  Search the products on the browser using API URLs to fetch product info.\n",
    " 3.  Understand elements in the response from the webiste API call to get product info.\n",
    " 4.  Program to fetch product details like name, quantity, price, discountprice if any and save the data to csv files.\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Products and Categories to be scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories to be scrapped  30\n",
      "Total products to be scrapped  142\n"
     ]
    }
   ],
   "source": [
    "df_products = pd.read_csv(\"products.csv\" )\n",
    "print(\"Total categories to be scrapped \", len(df_products[\"Product\"].unique()))  \n",
    "print(\"Total products to be scrapped \", df_products.shape[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrapping DMart\n",
    "\n",
    " - We need to pass the page number to the the URL for DMart.\n",
    " - We are limiting scrapting to 2 pages\n",
    " - DMart requires a Store Id be sent in the header to be sent. Otherwise, it will return wrong results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dmart Scrapping\n",
    "def findProducts_dmart(search_brand, search_product):\n",
    "    #Max pages to scrap\n",
    "    max_pages = 2\n",
    "    #Product full name\n",
    "    searchStr = search_brand + \" \" + search_product\n",
    "    #URL for page \n",
    "    url_page  = \"https://digital.dmart.in/api/v1/search/{0}?page={1}\"\n",
    "    #Necessary request headers\n",
    "    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "    \n",
    "    #DMart requires store Id \n",
    "    headers['storeId']='10657'\n",
    "    #Replace spaces in the product name with +\n",
    "    search_str_encoded = searchStr.replace(\" \",\"+\")\n",
    "    \n",
    "    #Extract product data into a dictionary object\n",
    "    product_data=[]\n",
    "    for page in range(1,max_pages):\n",
    "        url = url_page.format(search_str_encoded, page)\n",
    "        #print(url)\n",
    "        productsInfo = requests.get(url, headers=headers).json()\n",
    "        #If the scrapping did not yield any products, just continue with next product\n",
    "        products=productsInfo.get( 'suggestionView')\n",
    "        if(not products):\n",
    "            continue\n",
    "     \n",
    "        for product in  products:\n",
    "            skus = product['skus']\n",
    "            for sku in skus:  \n",
    "                if(sku['defining']):\n",
    "                    name =   sku.get( 'name',  product['name']).strip()\n",
    "                    product_data.append({\"search_brand\": search_brand, \"search_product\": search_product, \"shop\": \"DMart\",   \"product_name\": name,\"weight\": sku['defining'][0]['volume'], \"mrp\": sku['price_MRP'], \"special_price\": sku['price_SALE'] })\n",
    "    #Return the scrapped data           \n",
    "    return product_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrapping Grofers\n",
    " - We need to pass the page number to the the URL for Grofers.\n",
    " - Grofers requires a special header to be sent\n",
    " - We are limiting scrapting to 2 pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grofers Scrapping\n",
    "def findProducts_grofers(search_brand, search_product):\n",
    "    #Max pages to scrap\n",
    "    max_pages = 2\n",
    "    #Product full name\n",
    "    searchStr = search_brand + \" \" + search_product\n",
    "    #URL for page. This should include Longitude and Latitude as well. \n",
    "    url_page  = \"https://grofers.com/v5/search/merchants/26659/products/?lat=17.4196281427546&lon=78.3790778036223&q={0}&suggestion_type=0&t=1&start={1}&size=48\"\n",
    "    #Necessary request headers\n",
    "    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "    #Grofers requires this header \n",
    "    headers['app_client'] = 'consumer_web' \n",
    "    #Replace spaces in the product name with +\n",
    "    search_str_encoded = searchStr.replace(\" \",\"+\")\n",
    "    #Extract product data into a dictionary object\n",
    "    product_data=[]\n",
    "    for page in range(1,max_pages):\n",
    "        start_pos = (page-1)*48\n",
    "        if(start_pos==0):\n",
    "            start_pos = 1\n",
    "        #Create page specific URL\n",
    "        url = url_page.format(search_str_encoded, start_pos)\n",
    "        productsInfo = requests.get(url, headers=headers).json()\n",
    "        #If the scrapping did not yield any products, just continue with next product\n",
    "        products= productsInfo.get('products')\n",
    "        if(not products):\n",
    "            break\n",
    "        for product in  products:\n",
    "            prod_variants = product['variant_info']\n",
    "            for var in prod_variants:\n",
    "                product_data.append({\"search_brand\": search_brand, \"search_product\": search_product,   \"shop\": \"Grofers\",  \"product_name\": var[\"line_1\"],\"weight\": var[\"unit\"], \"mrp\": var[\"mrp\"], \"special_price\": var[\"price\"]})\n",
    "    #Return the scrapped data    \n",
    "    return product_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrapping Bigbasket\n",
    "\n",
    "- We need to maintain two separate URLs in the code as Big basket has different URLS for first page and other pages.\n",
    "- We are limiting scrapting to 2 pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Big Basket Scrapping\n",
    "def findProducts_bigbasket(search_brand, search_product):\n",
    "    \n",
    "    #Max pages to scrap\n",
    "    max_pages = 2\n",
    "    \n",
    "    #Product full name\n",
    "    searchStr = search_brand + \" \" + search_product\n",
    "    \n",
    "    #URL for page 1\n",
    "    url_page_1 = \"https://www.bigbasket.com/custompage/getsearchdata/?type=deck&slug={0}\"\n",
    "    \n",
    "    #URL for subsequent pages\n",
    "    url_for_page_n = \"https://www.bigbasket.com/product/get-products/?slug={0}&page={1}&tab_type=[%22all%22]&sorted_on=relevance&listtype=ps\"\n",
    "    \n",
    "    #Replace spaces in the product name with +\n",
    "    search_str_encoded = searchStr.replace(\" \",\"+\")\n",
    "    \n",
    "    #Form actual URL for page 1\n",
    "    url = url_page_1.format(search_str_encoded)\n",
    " \n",
    "    #Necessary request headers\n",
    "    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "    \n",
    "    #Get the response for page 1\n",
    "    productsInfo = requests.get(url, headers=headers).json()\n",
    "    \n",
    "    #Extract product data into a dictionary object\n",
    "    products =  productsInfo['json_data']['tab_info'][0]['product_info']['products']\n",
    "    product_data=[]\n",
    "    for product in  products:\n",
    "        options = product['all_prods']\n",
    "        if(not options): \n",
    "            product_data.append({\"search_brand\": search_brand, \"search_product\": search_product, \"shop\": \"Big Basket\", \"product_name\": str(product['p_brand']).strip() + \" \" + str(product['p_desc']).strip(), \"weight\":product['w'], \"mrp\": product['mrp'], \"special_price\": product['sp'] })\n",
    "            continue\n",
    "        for option  in product['all_prods']:\n",
    "            product_data.append({\"search_brand\": search_brand, \"search_product\": search_product, \"shop\": \"Big Basket\",  \"product_name\": str(product['p_brand']).strip() + \" \" + str(product['p_desc']).strip(), \"weight\":option['w'],\n",
    "                                 \"mrp\": option['mrp'], \"special_price\": option['sp'] }) \n",
    " \n",
    "    \n",
    "    #Do it for subsequent pages     \n",
    "    for page in range(2,max_pages):\n",
    "        newurl = url_for_page_n.format(search_str_encoded, page) \n",
    "        productsInfo = requests.get(newurl, headers=headers).json()\n",
    "        products =  productsInfo['tab_info']['product_map']['all']['prods']\n",
    "\n",
    "        for product in  products:\n",
    "            options = product['all_prods']\n",
    "            if(not options): \n",
    "                product_data.append({\"search_brand\": search_brand, \"search_product\": search_product, \"shop\": \"Big Basket\",    \"product_name\": str(product['p_brand']).strip() + \" \" + str(product['p_desc']).strip(), \"weight\":product['w'], \"mrp\": product['mrp'], \"special_price\": product['sp'] })\n",
    "                continue\n",
    "            for option  in product['all_prods']:\n",
    "                product_data.append({\"search_brand\": search_brand, \"search_product\": search_product,  \"shop\": \"Big Basket\",  \"product_name\": str(product['p_brand']).strip() + \" \" + str(product['p_desc']).strip(), \"weight\":option['w'], \"mrp\": option['mrp'], \"special_price\": option['sp'] })\n",
    "\n",
    "    #Return the scrapped data           \n",
    "    return product_data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Do Web Scrapping for all 3 websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping for all sites\n",
    "def scrapeAll():\n",
    "    #Read all items for which scrapping is needed\n",
    "    df_input   = pd.read_csv(\"products.csv\" )\n",
    "    #Define an empty dataframe to store results for each of the shops\n",
    "    column_names =   [\"search_brand\",\"search_product\", \"shop\",   \"product_name\", \"weight\", \"mrp\", \"special_price\"]\n",
    "    \n",
    "    df_bigbasket = pd.DataFrame(columns = column_names)\n",
    "    df_dmart = pd.DataFrame(columns = column_names)\n",
    "    df_grofers = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    #For each of the products, peform scrapping\n",
    "    for ( idx , search_brand, search_product) in df_input.itertuples():\n",
    "        print('Scrapping DMart for ', idx, search_brand, search_product)\n",
    "        product_data = findProducts_dmart(search_brand, search_product)\n",
    "        df_dmart = df_dmart.append(product_data[:3])\n",
    "        \n",
    "        print('Scrapping Grofers for ', idx, search_brand, search_product)\n",
    "        product_data = findProducts_grofers(search_brand, search_product)\n",
    "        df_grofers = df_grofers.append(product_data[:3])   \n",
    "        \n",
    "        print('Scrapping BigBasket for ', idx, search_brand, search_product)\n",
    "        product_data = findProducts_bigbasket(search_brand, search_product)\n",
    "        df_bigbasket = df_bigbasket.append(product_data[:3])\n",
    "\n",
    "       \n",
    "    #Store final results for each of the stores in separate csv file\n",
    "    df_dmart.to_csv('dmartDailyEssentails.csv', index=False, encoding='utf-8')\n",
    "    df_grofers.to_csv('grofersDailyEssentails.csv', index=False, encoding='utf-8')\n",
    "    df_bigbasket.to_csv('big_basketDailyEssentails.csv', index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scrapping Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping BigBasket for  0 3 Roses Tea\n",
      "Scrapping Grofers for  0 3 Roses Tea\n",
      "Scrapping BigBasket for  0 3 Roses Tea\n",
      "Scrapping BigBasket for  1 7 Up Soft Drink\n",
      "Scrapping Grofers for  1 7 Up Soft Drink\n",
      "Scrapping BigBasket for  1 7 Up Soft Drink\n",
      "Scrapping BigBasket for  2 ACT II Popcorn\n",
      "Scrapping Grofers for  2 ACT II Popcorn\n",
      "Scrapping BigBasket for  2 ACT II Popcorn\n",
      "Scrapping BigBasket for  3 Adidas Deodrant\n",
      "Scrapping Grofers for  3 Adidas Deodrant\n",
      "Scrapping BigBasket for  3 Adidas Deodrant\n",
      "Scrapping BigBasket for  4 Anurag Cooking Oil\n",
      "Scrapping Grofers for  4 Anurag Cooking Oil\n",
      "Scrapping BigBasket for  4 Anurag Cooking Oil\n",
      "Scrapping BigBasket for  5 Appy Fizz Juice\n",
      "Scrapping Grofers for  5 Appy Fizz Juice\n",
      "Scrapping BigBasket for  5 Appy Fizz Juice\n",
      "Scrapping BigBasket for  6 Aswini Hair Oil\n",
      "Scrapping Grofers for  6 Aswini Hair Oil\n",
      "Scrapping BigBasket for  6 Aswini Hair Oil\n",
      "Scrapping BigBasket for  7 Bisleri Mineral Water\n",
      "Scrapping Grofers for  7 Bisleri Mineral Water\n",
      "Scrapping BigBasket for  7 Bisleri Mineral Water\n",
      "Scrapping BigBasket for  8 Boost Soft Drink\n",
      "Scrapping Grofers for  8 Boost Soft Drink\n",
      "Scrapping BigBasket for  8 Boost Soft Drink\n",
      "Scrapping BigBasket for  9 Britannia Rusks\n",
      "Scrapping Grofers for  9 Britannia Rusks\n",
      "Scrapping BigBasket for  9 Britannia Rusks\n",
      "Scrapping BigBasket for  10 Brooke Bond Tea\n",
      "Scrapping Grofers for  10 Brooke Bond Tea\n",
      "Scrapping BigBasket for  10 Brooke Bond Tea\n",
      "Scrapping BigBasket for  11 Bru Coffee\n",
      "Scrapping Grofers for  11 Bru Coffee\n",
      "Scrapping BigBasket for  11 Bru Coffee\n",
      "Scrapping BigBasket for  12 Cadbury Dairy milk Chocolate\n",
      "Scrapping Grofers for  12 Cadbury Dairy milk Chocolate\n",
      "Scrapping BigBasket for  12 Cadbury Dairy milk Chocolate\n",
      "Scrapping BigBasket for  13 Cadbury Five star Chocolate\n",
      "Scrapping Grofers for  13 Cadbury Five star Chocolate\n",
      "Scrapping BigBasket for  13 Cadbury Five star Chocolate\n",
      "Scrapping BigBasket for  14   Chana Dal\n",
      "Scrapping Grofers for  14   Chana Dal\n",
      "Scrapping BigBasket for  14   Chana Dal\n",
      "Scrapping BigBasket for  15 Cinthol Bath Soap\n",
      "Scrapping Grofers for  15 Cinthol Bath Soap\n",
      "Scrapping BigBasket for  15 Cinthol Bath Soap\n",
      "Scrapping BigBasket for  16 Cinthol Deodrant\n",
      "Scrapping Grofers for  16 Cinthol Deodrant\n",
      "Scrapping BigBasket for  16 Cinthol Deodrant\n",
      "Scrapping BigBasket for  17 Clinic Plus Shampoo\n",
      "Scrapping Grofers for  17 Clinic Plus Shampoo\n",
      "Scrapping BigBasket for  17 Clinic Plus Shampoo\n",
      "Scrapping BigBasket for  18 Close Up Tooth Paste\n",
      "Scrapping Grofers for  18 Close Up Tooth Paste\n",
      "Scrapping BigBasket for  18 Close Up Tooth Paste\n",
      "Scrapping BigBasket for  19 Coca Cola Soft Drink\n",
      "Scrapping Grofers for  19 Coca Cola Soft Drink\n",
      "Scrapping BigBasket for  19 Coca Cola Soft Drink\n",
      "Scrapping BigBasket for  20 Colgate MaxFresh Tooth Paste\n",
      "Scrapping Grofers for  20 Colgate MaxFresh Tooth Paste\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-56beeeb29246>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Call scrapping routine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mperform_scrapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#Load scrapped data into data frames and look at the metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_bb\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"big_basketDailyEssentails.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_dmart\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dmartDailyEssentails.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-96247644f44c>\u001b[0m in \u001b[0;36mperform_scrapping\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Scrapping Grofers for '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_brand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_product\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mproduct_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_grofers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_brand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_product\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mdf_grofers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_grofers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Scrapping BigBasket for '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_brand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_product\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   7106\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7107\u001b[0m                 \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7108\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7109\u001b[0m             \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Call scrapping routine\n",
    "scrapeAll()\n",
    "#Load scrapped data into data frames and look at the metrics\n",
    "df_dmart  = pd.read_csv(\"dmartDailyEssentails.csv\")\n",
    "df_grofers  = pd.read_csv(\"grofersDailyEssentails.csv\")\n",
    "df_bb  = pd.read_csv(\"big_basketDailyEssentails.csv\")\n",
    "\n",
    "print(\"DMart returned products count:\" , df_dmart.shape[0] )\n",
    "print(\"Grofers returned products count:\" , df_grofers.shape[0] ) \n",
    "print(\"Bigbasket returned products count:\" , df_bb.shape[0] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files generated by WebScraping has raw data. \n",
    "This data has to be cleaned by identiying below issues and fixing them and saving to new csv files.\n",
    "    1. Removing Noise or special characters\n",
    "    2. Missing Values Imputation\n",
    "    3. Deriving Quantity from product descriptions and discount from MRP and SP.\n",
    "    4. Removing Duplicate records.\n",
    "    5. Arrange all columns in same order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cleaning up for DMart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dmartDailyEssentails.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-998f5538b889>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dmartDailyEssentails.csv\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1145\u001b[0m                     \u001b[1;34m' \"python-fwf\")'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m                 )\n\u001b[1;32m-> 1147\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m         )\n\u001b[0;32m   2295\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dmartDailyEssentails.csv'"
     ]
    }
   ],
   "source": [
    "df  = pd.read_csv(\"dmartDailyEssentails.csv\",  engine='python')\n",
    "print(df.product_name.unique())\n",
    "print(df.weight.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below issues will be fixed on Dmart data.\n",
    " - There are products with combo packs. So, remove them as identifying individual product becomes tough during recommendation\n",
    " - Remove products with weights that has invalid characters other than numbers\n",
    " - Drop duplicate products that may have arrived due to wrong search results given by the site\n",
    " - Product name contains weight. Remove it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cleaning up Big Basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(\"big_basketDailyEssentials.csv\",  engine='python')\n",
    "print(df.product_name.unique())\n",
    "print(df.weight.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below issues will be fixed on BigBasket data.\n",
    "\n",
    " - There are products with combo packs. So, remove them as identifying individual product becomes tough during recommendation\n",
    " - Remove products with weights that has invalid characters other than numbers\n",
    " - Drop duplicate products that may have arrived due to wrong search results given by the site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup routine for bigbasket\n",
    "def clean_bigbasket_data():\n",
    "    #Read the dataset\n",
    "    df  = pd.read_csv(\"big_basket.csv\",  engine='python')\n",
    "    # Pattern to identify combo packs\n",
    "    searchfor = ['x', 'X','\\+','Comb',  'each', '\\(']\n",
    "    #Drop the rows with above patterns\n",
    "    df.drop( df[ df[\"weight\"].str.contains('|'.join(searchfor)) ].index,inplace=True  )\n",
    "    #Separate weight in to weight, measure\n",
    "    df[['weight','measure']] = df.weight.str.split(expand=True) \n",
    "    #Remove rows that have invalid weight entries\n",
    "    df.drop( df[~df[\"weight\"].str.replace('.','',1).str.isnumeric() ].index,inplace=True  )\n",
    "    #Check if product name has the keyword we searched for\n",
    "    df['good_product_ind'] = [x[1] in x[0] for x in zip(df['product_name'], df['search_brand'])]\n",
    "    #Drop the products that don;t meet the above criteria\n",
    "    df.drop( df[~df[\"good_product_ind\"] ].index,inplace=True  )   \n",
    "    #Creae weight_measure column by combining weight and measure\n",
    "    df[ 'weight_measure' ] = df.weight.str.strip() + df.measure.str.strip()\n",
    "    #Re-order columns\n",
    "    df = df[['search_product','search_brand', 'shop' ,'product_name','weight', 'measure', 'weight_measure', 'mrp','special_price']]\n",
    "    #Calculate discount\n",
    "    df['discount'] = (df.mrp - df.special_price) *100 / df.mrp \n",
    "    #Remove duplicate entries for given product and weight\n",
    "    df.drop_duplicates(subset=['product_name','weight'], keep='last', inplace=True)\n",
    "    #Save the results to big_basket_cleaned\n",
    "    df.to_csv('big_basket_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cleaning up for Grofers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(\"grofers.csv\",  engine='python')\n",
    "print(df.product_name.unique())\n",
    "print(df.weight.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below issues will be fixed for Grofers data\n",
    "\n",
    " - There are products with combo packs. So, remove them as identifying individual product becomes tough during recommendation\n",
    " - Remove products with weights that has invalid characters other than numbers\n",
    " - Drop duplicate products that may have arrived due to wrong search results given by the site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup routine for grofers\n",
    "def clean_grofers_data():\n",
    "    #Read the dataset\n",
    "    df  = pd.read_csv(\"grofers.csv\")\n",
    "    # Pattern to identify combo packs\n",
    "    searchfor = ['x', 'X', ',', 'Refills', 'Bags','Sachets','Drops','Wipes','U','Wipe','Pellets','Cubes','unit','units','tablets',\"\\+\",'XL','L','M','S','Set','Pink']\n",
    "    #Drop the rows with above patterns\n",
    "    df.drop( df[ df[\"weight\"].str.contains('|'.join(searchfor)) ].index,inplace=True  )\n",
    "    #Separate weight in to weight, measure\n",
    "    df[['weight','measure']] = df.weight.str.split(expand=True) \n",
    "    #Remove rows that have invalid weight entries\n",
    "    df.drop( df[~df[\"weight\"].str.replace('.','',1).str.isnumeric() ].index,inplace=True  )\n",
    "    #Creae weight_measure column by combining weight and measure\n",
    "    df[ 'weight_measure' ] = df.weight.str.strip() + df.measure.str.strip()\n",
    "    #Check if product name has the keyword we searched for\n",
    "    df['good_product_ind'] = [x[1] in x[0] for x in zip(df['product_name'], df['search_brand'])]\n",
    "    #Drop the products that don;t meet the above criteria\n",
    "    df.drop( df[~df[\"good_product_ind\"] ].index,inplace=True  )    \n",
    "    #Re-order columns\n",
    "    df = df[['search_product','search_brand', 'shop' ,'product_name','weight', 'measure', 'weight_measure', 'mrp','special_price']]\n",
    "    #Calculate discount\n",
    "    df['discount'] = (df.mrp - df.special_price) *100 / df.mrp \n",
    "    #Remove duplicate entries for given product and weight\n",
    "    df.drop_duplicates(subset=[  'product_name','weight'],keep='last', inplace=True)\n",
    "    #Save the results to grofers_cleaned\n",
    "    df.to_csv('grofers_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cleaning up all datasets at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_dmart_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-30b99b9e28bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Run cleanup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclean_dmart_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mclean_grofers_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclean_bigbasket_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_dmart_data' is not defined"
     ]
    }
   ],
   "source": [
    "#Run cleanup \n",
    "clean_dmart_data()\n",
    "clean_grofers_data()\n",
    "clean_bigbasket_data()\n",
    "\n",
    "df_dmart  = pd.read_csv(\"dmart_cleaned.csv\")\n",
    "df_grofers  = pd.read_csv(\"grofers_cleaned.csv\")\n",
    "df_bb  = pd.read_csv(\"big_basket_cleaned.csv\")\n",
    "\n",
    "print(\"DMart products count after cleanup:\" , df_dmart.shape[0] )\n",
    "print(\"Grofers products count after cleanup:\" , df_grofers.shape[0] ) \n",
    "print(\"Bigbasket products count after cleanup:\" , df_bb.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps followed:\n",
    "1. All the cleaned files from 3 websited will be loaded into 3 dataframes and will be concatenated into a single dataframe.\n",
    "2. All products will be sorted on the product type and name.\n",
    "3. The final dataframe is converted into csv for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine cleaned files into combined.csv file\n",
    "def integrate_products():\n",
    "    #Load cleaned files into separate dataframes \n",
    "    df_dmart  = pd.read_csv(\"dmart_cleaned.csv\")\n",
    "    df_grofers  = pd.read_csv(\"grofers_cleaned.csv\")\n",
    "    df_bb  = pd.read_csv(\"big_basket_cleaned.csv\")\n",
    "   \n",
    "    #Combine these files into a single data frame\n",
    "    df_combined = pd.concat([df_bb, df_dmart, df_grofers])\n",
    "    \n",
    "    #Sort them based on searched product and resultant product name\n",
    "    df_combined.sort_values(by=[ 'search_product' , 'product_name'], inplace=True)\n",
    "    #Reset the index\n",
    "    df_combined = df_combined.reset_index(drop=True)\n",
    "    #Store the results in final file combined.csv\n",
    "    df_combined.to_csv('combined.csv', index=False)\n",
    "     \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Products \n",
    "integrate_products()\n",
    "#Read combined products into data frame\n",
    "df_combined  = pd.read_csv(\"combined.csv\")\n",
    "print(\"Final Scrapped and Cleaned Product Count:\" , df_combined.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. EXPLORATORY DATA ANALYSIS & RECOMMENDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps followed:\n",
    "1. Final dataframe of combined csv and various graphs are plotted for vizualization and inferences are made.\n",
    "2. Best Recommendation provided for a chosen product on the basis of least price by checking discount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file into a dataframe\n",
    "items = pd.read_csv('combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print shape of the combined dataset\n",
    "print(\"The total count of products available across shops \",  items.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the first few rows\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are any null values in the final dataset\n",
    "items.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which shops are part of the dataset\n",
    "items.shop.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique product categories\n",
    "items.search_product.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot for all attributes\n",
    "sns.pairplot(items)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap for all attributes\n",
    "plt.figure(figsize = (10,8))\n",
    "corr = items.corr()\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Inference: The above heat map shows there is a tight correlation between mrp and special_price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many products does each shop have to offer?\n",
    "prods_shops_df = items[['shop','search_product']]\n",
    "prod_shops_counts =prods_shops_df.groupby(['shop'],as_index =False).count()\n",
    "prod_shops_counts.plot.bar(x=\"shop\", y=\"search_product\", rot=70, title=\"Products accross Vendors\");\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Inference: Big Basket has highest number of products available while Grofers has the least. Customers are most likely to find a product in Big Basket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many products avaialble for each product category overall\n",
    "products_count =prods_shops_df.groupby(['search_product'],as_index =False).count()\n",
    "dataFrame = pd.DataFrame(data=products_count);\n",
    "y_pos = np.arange(len(dataFrame))\n",
    "# Draw a vertical bar chart\n",
    "ax = dataFrame.plot.bar(x=\"search_product\", y=\"shop\", rot=80, title=\"Products and their Availability\");\n",
    "ax.set_xlabel(\"Product\")\n",
    "ax.set_ylabel(\"Count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Number of products sold per product category by each shop \n",
    "pd.crosstab(items.shop, items.search_product, margins=True, margins_name=\"Total\",rownames=['shop']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Which Individual Products  have Discount\n",
    "discounts_df = items[ (items.discount > 0.0)]\n",
    "  \n",
    "discounts_shop_products = discounts_df[['search_product','shop']]\n",
    "discounts_per_shop =discounts_shop_products.groupby(['shop'],as_index =False).count()\n",
    "ax = discounts_per_shop.plot.bar(x=\"shop\", y=\"search_product\", rot=70, title=\"Products accross Vendors with discount\");\n",
    "ax.set_xlabel(\"Shop\")\n",
    "ax.set_ylabel(\"Product count with discount\") \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Inference: DMart offers the most discounts. So customers looking for discounts should visit DMart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure combined.csv file is present in current path before executing following recommendation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'combined.csv' does not exist: b'combined.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-d7327594901d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#Read data into dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf_combined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"combined.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#Unique product types that the user can buy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'combined.csv' does not exist: b'combined.csv'"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries for user interface\n",
    "from ipywidgets import interact, Dropdown, HTML, Layout, Box, Label\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Read data into dataframe\n",
    "df_combined = pd.read_csv(\"combined.csv\")\n",
    "\n",
    "#Unique product types that the user can buy\n",
    "products_list = df_combined[\"search_product\"].unique()     \n",
    "\n",
    "#Create dropdown widgets for the user to interact\n",
    "product_type_dropdown = Dropdown(description=\"Product Type:\", options = products_list)\n",
    "product_names_dropdown = Dropdown(description=\"Product:\")\n",
    "product_sizes_dropdown = Dropdown(description=\"Size:\")\n",
    "recommendation_html = HTML(\n",
    "    value=\" \",\n",
    "    placeholder='',\n",
    "    description='',  layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "box = Box(\n",
    "    [\n",
    "        Label(value='Recommendation:'),\n",
    "        recommendation_html\n",
    "    ]\n",
    ") \n",
    "#Event handler when product category changes\n",
    "def update_product_name_options(change):  \n",
    "    product_type = product_type_dropdown.value         \n",
    "    df_product = df_combined[  df_combined.search_product.str.lower().str.contains(product_type.lower())    ]\n",
    "    products = df_product[\"product_name\"].unique()\n",
    "    product_names_dropdown.options = products\n",
    "product_type_dropdown.observe(update_product_name_options )\n",
    " \n",
    "#Event handler when product name changes\n",
    "def update_product_size_options(change): \n",
    "    if change['name'] == 'value' and (change['new'] != change['old']):\n",
    "        product_type = product_type_dropdown.value\n",
    "        product_name = product_names_dropdown.value\n",
    "        df_product = df_combined[  df_combined.search_product.str.lower().str.contains(product_type.lower())    ]\n",
    "        product_name = product_name.replace(\"+\", \"\\+\")\n",
    "        df_product = df_product[  df_product.product_name.str.lower().str.contains(product_name.lower())   ] \n",
    "        product_sizes = df_product[\"weight_measure\"].unique()\n",
    "        product_sizes_dropdown.options = product_sizes\n",
    "product_names_dropdown.observe(update_product_size_options )\n",
    "\n",
    "#Event handler when product size changes\n",
    "def recommend_product(change):\n",
    "    if change['name'] == 'value' and (change['new'] != change['old']):\n",
    "        product_type = product_type_dropdown.value\n",
    "        product_name = product_names_dropdown.value\n",
    "        product_size = product_sizes_dropdown.value\n",
    "        recommendation_html.value=\"\"\n",
    "        product_name = product_name.replace(\"+\", \"\\+\")\n",
    "        if(product_size):\n",
    "            df_final_list = df_combined[ df_combined.search_product.str.lower().str.contains(product_type.lower()) & \n",
    "                                         df_combined.product_name.str.lower().str.contains(product_name.lower()) & \n",
    "                                         df_combined.weight_measure.str.lower().str.contains(product_size.lower()) ] \n",
    "            df_final_list.sort_values(by='special_price', inplace=True)\n",
    "            discount = round(  df_final_list[ \"discount\"].iloc[0] , 2)\n",
    "            recommendation_html.value= \"Buy it from <b>\" + df_final_list.shop.iloc[0] + \"</b> for Rs.\"  + str(df_final_list[ \"special_price\"].iloc[0])  + \" discount of \" + str(discount) + \"%\"\n",
    "product_sizes_dropdown.observe(recommend_product )\n",
    "\n",
    "#Display all user interface elements \n",
    "display(product_type_dropdown)\n",
    "display(product_names_dropdown)\n",
    "display(product_sizes_dropdown)\n",
    "display(box)\n",
    "update_product_name_options(None )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
